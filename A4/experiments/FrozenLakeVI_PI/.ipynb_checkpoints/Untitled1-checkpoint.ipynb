{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea915f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92d0acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.frozen_lake import generate_random_map, FrozenLakeEnv\n",
    "np.random.seed(2)\n",
    "sixteen = generate_random_map(16)\n",
    "np.random.seed(44)\n",
    "tvelve = generate_random_map(12)\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "956ac682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, n_epoch=1000):\n",
    "    rewards = []\n",
    "    episode_counts = []\n",
    "    for i in range(n_epoch):\n",
    "        current_state = env.reset()  # Directly use the returned state\n",
    "        ep = 0\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done and ep < 1000:\n",
    "            ep += 1\n",
    "            act = int(policy[current_state])\n",
    "            new_state, reward, done, _ = env.step(act)  # Unpacking the result directly\n",
    "            current_state = new_state  # Updating current_state for the next iteration\n",
    "            episode_reward += reward\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        episode_counts.append(ep)\n",
    "\n",
    "    mean_reward = sum(rewards)/len(rewards)\n",
    "    mean_eps = sum(episode_counts)/len(episode_counts)\n",
    "    return mean_reward, mean_eps, rewards, episode_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9b3650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount=0.9, epsilon=1e-12):\n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.zeros(number_of_states)\n",
    "    value_list = np.zeros(number_of_states)\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = 1\n",
    "    sigma = discount\n",
    "\n",
    "    while max_change > epsilon:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = old_value_list[new_state]\n",
    "                    cand_value = reward + sigma*value_new_state if not done else reward\n",
    "                    total_cand_value += cand_value*prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[s] = a\n",
    "                    value_list[s] = assigned_value\n",
    "\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "        \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy, episode, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e04dace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount=0.9, epsilon=1e-3):\n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.random.randint(number_of_actions, size=number_of_states)\n",
    "    value_list = np.zeros(number_of_states)\n",
    "    episode = 0\n",
    "    sigma = discount\n",
    "\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                v = value_list[s]\n",
    "                a = policy[s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[new_state]\n",
    "                    cand_value = reward + sigma*value_new_state if not done else reward\n",
    "                    total_val_new_state += cand_value*prob \n",
    "                value_list[s] = total_val_new_state\n",
    "                eps = max(eps, np.abs(v - value_list[s]))\n",
    "            if eps < epsilon:\n",
    "                eval_acc = False\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "            old_action = policy[s]\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[new_state]\n",
    "                    cand_value = reward + sigma*value_new_state if not done else reward\n",
    "                    total_cand_value += prob*cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[s] = a\n",
    "\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy, episode, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3d7cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_pi_vi(env, discount=[0.9], epsilon=[1e-9], mute=False):\n",
    "    \n",
    "    vi_dict = {}\n",
    "    \n",
    "    for dis in discount:\n",
    "        vi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            vi_dict[dis][eps] = {}\n",
    "            \n",
    "            # run value iteration\n",
    "            vi_policy, vi_solve_iter, vi_solve_time = value_iteration(env, dis, eps)\n",
    "            vi_mrews, vi_meps, _, __ = test_policy(env, vi_policy)    \n",
    "            vi_dict[dis][eps][\"mean_reward\"] = vi_mrews\n",
    "            vi_dict[dis][eps][\"mean_eps\"] = vi_meps\n",
    "            vi_dict[dis][eps][\"iteration\"] = vi_solve_iter\n",
    "            vi_dict[dis][eps][\"time_spent\"] = vi_solve_time\n",
    "            vi_dict[dis][eps][\"policy\"] = vi_policy\n",
    "            if not mute:\n",
    "                print(\"Value iteration for {} discount and {} eps is done\".format(dis, eps))\n",
    "                print(\"Iteration: {} time: {}\".format(vi_solve_iter, vi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(vi_mrews, vi_meps))\n",
    "    # run policy iteration\n",
    "    pi_dict = {}\n",
    "    for dis in discount:\n",
    "        pi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            pi_dict[dis][eps] = {}\n",
    "\n",
    "            pi_policy, pi_solve_iter, pi_solve_time = policy_iteration(env, dis, eps)\n",
    "            pi_mrews, pi_meps, _, __ = test_policy(env, pi_policy)    \n",
    "            pi_dict[dis][eps][\"mean_reward\"] = pi_mrews\n",
    "            pi_dict[dis][eps][\"mean_eps\"] = pi_meps\n",
    "            pi_dict[dis][eps][\"iteration\"] = pi_solve_iter\n",
    "            pi_dict[dis][eps][\"time_spent\"] = pi_solve_time\n",
    "            pi_dict[dis][eps][\"policy\"] = pi_policy\n",
    "            if not mute:\n",
    "                print(\"Policy iteration for {} discount is done\".format(dis))\n",
    "                print(\"Iteration: {} time: {}\".format(pi_solve_iter, pi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(pi_mrews, pi_meps))\n",
    "\n",
    "    \n",
    "    return vi_dict, pi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ba68eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_discretize(the_map):\n",
    "    size = len(the_map)\n",
    "    dis_map = np.zeros((size,size))\n",
    "    for i, row in enumerate(the_map):\n",
    "        for j, loc in enumerate(row):\n",
    "            if loc == \"S\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"F\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"H\":\n",
    "                dis_map[i, j] = -1\n",
    "            elif loc == \"G\":\n",
    "                dis_map[i, j] = 1\n",
    "    return dis_map\n",
    "\n",
    "\n",
    "def policy_numpy(policy):\n",
    "    size = int(np.sqrt(len(policy)))\n",
    "    pol = np.asarray(policy)\n",
    "    pol = pol.reshape((size, size))\n",
    "    return pol\n",
    "\n",
    "\n",
    "def see_policy(map_size, policy):\n",
    "    map_name = str(map_size)+\"x\"+str(map_size)\n",
    "    data = map_discretize(MAPS[map_name])\n",
    "    np_pol = policy_numpy(policy)\n",
    "    plt.imshow(data, interpolation=\"nearest\")\n",
    "\n",
    "    for i in range(np_pol[0].size):\n",
    "        for j in range(np_pol[0].size):\n",
    "            arrow = '\\u2190'\n",
    "            if np_pol[i, j] == 1:\n",
    "                arrow = '\\u2193'\n",
    "            elif np_pol[i, j] == 2:\n",
    "                arrow = '\\u2192'\n",
    "            elif np_pol[i, j] == 3:\n",
    "                arrow = '\\u2191'\n",
    "            text = plt.text(j, i, arrow,\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "377cb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_dict(dictionary, value=\"Score\", size=4, variable=\"Discount Rate\", log=False):\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    title = \"Average and Max {} on {}x{} Frozen Lake\".format(value, size, size)\n",
    "    the_val = value\n",
    "    value = \"Average {}\".format(the_val)\n",
    "    val_type = \"Type of {}\".format(the_val)\n",
    "    the_df = pd.DataFrame(columns=[variable, value, val_type])\n",
    "    for k, v in dictionary.items():\n",
    "        for val in v:\n",
    "            if not log:\n",
    "                dic = {variable: k, value: float(val), val_type: \"Average with std\"}\n",
    "            else:\n",
    "                dic = {variable: np.log10(k), value: float(val), val_type: \"Average with std\"}                \n",
    "            the_df = the_df.append(dic, ignore_index=True)\n",
    "        if not log:\n",
    "            dic = {variable: k, value: float(max(v)), val_type: \"Max\"}\n",
    "        else:\n",
    "            dic = {variable: np.log10(k), value: float(max(v)), val_type: \"Max\"}\n",
    "        the_df = the_df.append(dic, ignore_index=True)\n",
    "    sns.lineplot(x=variable, y=value, hue=val_type, style=val_type, markers=True, data=the_df).set(title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d30e6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dict(the_dict):\n",
    "    \n",
    "    # return for discount\n",
    "    discount_rewards = {}\n",
    "    discount_iterations = {}\n",
    "    discount_times = {}\n",
    "\n",
    "\n",
    "    for disc in the_dict:\n",
    "        discount_rewards[disc] = []    \n",
    "        discount_iterations[disc] = []    \n",
    "        discount_times[disc] = []\n",
    "\n",
    "        for eps in the_dict[disc]:\n",
    "            discount_rewards[disc].append(the_dict[disc][eps]['mean_reward'])\n",
    "            discount_iterations[disc].append(the_dict[disc][eps]['iteration'])        \n",
    "            discount_times[disc].append(the_dict[disc][eps]['time_spent'].total_seconds())  \n",
    "\n",
    "            \n",
    "    epsilon_rewards = {}\n",
    "    epsilon_iterations = {}\n",
    "    epsilon_times = {}\n",
    "    for eps in the_dict[0.5]:\n",
    "        epsilon_rewards[eps] = []    \n",
    "        epsilon_iterations[eps] = []    \n",
    "        epsilon_times[eps] = []\n",
    "    \n",
    "        for disc in vi_dict:\n",
    "            epsilon_rewards[eps].append(the_dict[disc][eps]['mean_reward'])\n",
    "            epsilon_iterations[eps].append(the_dict[disc][eps]['iteration'])        \n",
    "            epsilon_times[eps].append(the_dict[disc][eps]['time_spent'].total_seconds()) \n",
    "            \n",
    "    return discount_rewards, discount_iterations, discount_times, epsilon_rewards, epsilon_iterations, epsilon_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a6e78ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in: 7 episodes and 0:00:00.002615 seconds\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m vi_dict, pi_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test_pi_vi\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 12\u001b[0m, in \u001b[0;36mtrain_and_test_pi_vi\u001b[0;34m(env, discount, epsilon, mute)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# run value iteration\u001b[39;00m\n\u001b[1;32m     11\u001b[0m vi_policy, vi_solve_iter, vi_solve_time \u001b[38;5;241m=\u001b[39m value_iteration(env, dis, eps)\n\u001b[0;32m---> 12\u001b[0m vi_mrews, vi_meps, _, __ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvi_policy\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     13\u001b[0m vi_dict[dis][eps][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m vi_mrews\n\u001b[1;32m     14\u001b[0m vi_dict[dis][eps][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_eps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m vi_meps\n",
      "Cell \u001b[0;32mIn[90], line 11\u001b[0m, in \u001b[0;36mtest_policy\u001b[0;34m(env, policy, n_epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m ep \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m     10\u001b[0m     ep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     12\u001b[0m     new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(act)  \u001b[38;5;66;03m# Unpacking the result directly\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m new_state  \u001b[38;5;66;03m# Updating current_state for the next iteration\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "vi_dict, pi_dict = train_and_test_pi_vi(env, discount=[0.5, 0.75, 0.9, 0.95, 0.99, 0.9999], \n",
    "                                        epsilon=[1e-3, 1e-6, 1e-9, 1e-12, 1e-15], mute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31559e16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
