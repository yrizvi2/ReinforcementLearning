{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7f45675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map, FrozenLakeEnv\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14aecca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e5d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(env, policy, n_epoch=1000):\n",
    "    rewards = []\n",
    "    episode_counts = []\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        current_state = env.reset()\n",
    "        ep = 0\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done and ep < 1000:\n",
    "            ep += 1\n",
    "            action = policy[current_state]\n",
    "            act = int(action)\n",
    "            \n",
    "            # Adjusting unpacking to match the output of env.step()\n",
    "            step_output = env.step(act)\n",
    "            if isinstance(step_output, tuple) and len(step_output) == 5:\n",
    "                new_state, reward, done, is_slippery, _ = step_output\n",
    "            else:\n",
    "                print(\"Unexpected output from env.step:\", step_output)\n",
    "                break\n",
    "\n",
    "            episode_reward += reward\n",
    "            current_state = new_state\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        episode_counts.append(ep)\n",
    "\n",
    "    # ... rest of the function remains the same\n",
    "    # Calculate various metrics\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    min_reward = np.min(rewards)\n",
    "    max_reward = np.max(rewards)\n",
    "    mean_eps = np.mean(episode_counts)\n",
    "    \n",
    "    return {\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"min_reward\": min_reward,\n",
    "        \"max_reward\": max_reward,\n",
    "        \"mean_eps\": mean_eps,\n",
    "        \"rewards\": rewards,\n",
    "        \"episode_counts\": episode_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae7c9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount=0.9, epsilon=1e-12):\n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.zeros(number_of_states)\n",
    "    value_list = np.zeros(number_of_states)\n",
    "    old_value_list = value_list.copy()\n",
    "    episode = 0\n",
    "    max_change = np.inf  # Initialize max_change\n",
    "    sigma = discount\n",
    "\n",
    "    while max_change > epsilon:\n",
    "        episode += 1\n",
    "        for s in range(number_of_states):\n",
    "            assigned_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = old_value_list[new_state]\n",
    "                    cand_value = reward + sigma * value_new_state if not done else reward\n",
    "                    total_cand_value += cand_value * prob \n",
    "                        \n",
    "                if total_cand_value > assigned_value:\n",
    "                    assigned_value = total_cand_value\n",
    "                    policy[s] = a\n",
    "                    value_list[s] = assigned_value\n",
    "\n",
    "        changes = np.abs(value_list - old_value_list)\n",
    "        max_change = np.max(changes)\n",
    "        old_value_list = value_list.copy()\n",
    "        \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy, episode, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee3d86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount=0.9, epsilon=1e-3):\n",
    "    start = timer()\n",
    "    \n",
    "    number_of_states = env.observation_space.n\n",
    "    number_of_actions = env.action_space.n\n",
    "    policy = np.random.randint(number_of_actions, size=number_of_states)\n",
    "    value_list = np.zeros(number_of_states)\n",
    "    episode = 0\n",
    "    sigma = discount\n",
    "    \n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        episode += 1\n",
    "        eval_acc = True\n",
    "        while eval_acc:\n",
    "            eps = 0\n",
    "            for s in range(number_of_states):\n",
    "                v = value_list[s]\n",
    "                a = policy[s]\n",
    "                total_val_new_state = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[new_state]\n",
    "                    cand_value = reward + sigma * value_new_state if not done else reward\n",
    "                    total_val_new_state += cand_value * prob \n",
    "                value_list[s] = total_val_new_state\n",
    "                eps = max(eps, np.abs(v - value_list[s]))\n",
    "            if eps < epsilon:\n",
    "                eval_acc = False\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in range(number_of_states):\n",
    "            old_action = policy[s]\n",
    "            max_value = -np.inf\n",
    "            for a in range(number_of_actions):\n",
    "                total_cand_value = 0\n",
    "                for prob, new_state, reward, done in env.P[s][a]:\n",
    "                    value_new_state = value_list[new_state]\n",
    "                    cand_value = reward + sigma * value_new_state if not done else reward\n",
    "                    total_cand_value += prob * cand_value\n",
    "                if total_cand_value > max_value:\n",
    "                    max_value = total_cand_value\n",
    "                    policy[s] = a\n",
    "\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "    \n",
    "    end = timer()\n",
    "    time_spent = timedelta(seconds=end-start)\n",
    "    print(\"Solved in: {} episodes and {} seconds\".format(episode, time_spent))\n",
    "    return policy, episode, time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf4e2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_pi_vi(env, discount=[0.9], epsilon=[1e-9], mute=False):\n",
    "    \n",
    "    vi_dict = {}\n",
    "    \n",
    "    for dis in discount:\n",
    "        vi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            vi_dict[dis][eps] = {}\n",
    "            \n",
    "            # run value iteration\n",
    "            vi_policy, vi_solve_iter, vi_solve_time = value_iteration(env, dis, eps)\n",
    "            vi_mrews, vi_meps, _, __ = test_policy(env, vi_policy)    \n",
    "            vi_dict[dis][eps][\"mean_reward\"] = vi_mrews\n",
    "            vi_dict[dis][eps][\"mean_eps\"] = vi_meps\n",
    "            vi_dict[dis][eps][\"iteration\"] = vi_solve_iter\n",
    "            vi_dict[dis][eps][\"time_spent\"] = vi_solve_time\n",
    "            vi_dict[dis][eps][\"policy\"] = vi_policy\n",
    "            if not mute:\n",
    "                print(\"Value iteration for {} discount and {} eps is done\".format(dis, eps))\n",
    "                print(\"Iteration: {} time: {}\".format(vi_solve_iter, vi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(vi_mrews, vi_meps))\n",
    "    # run policy iteration\n",
    "    pi_dict = {}\n",
    "    for dis in discount:\n",
    "        pi_dict[dis] = {}\n",
    "        for eps in epsilon:\n",
    "            pi_dict[dis][eps] = {}\n",
    "\n",
    "            pi_policy, pi_solve_iter, pi_solve_time = policy_iteration(env, dis, eps)\n",
    "            pi_mrews, pi_meps, _, __ = test_policy(env, pi_policy)    \n",
    "            pi_dict[dis][eps][\"mean_reward\"] = pi_mrews\n",
    "            pi_dict[dis][eps][\"mean_eps\"] = pi_meps\n",
    "            pi_dict[dis][eps][\"iteration\"] = pi_solve_iter\n",
    "            pi_dict[dis][eps][\"time_spent\"] = pi_solve_time\n",
    "            pi_dict[dis][eps][\"policy\"] = pi_policy\n",
    "            if not mute:\n",
    "                print(\"Policy iteration for {} discount is done\".format(dis))\n",
    "                print(\"Iteration: {} time: {}\".format(pi_solve_iter, pi_solve_time))\n",
    "                print(\"Mean reward: {} - mean eps: {}\".format(pi_mrews, pi_meps))\n",
    "\n",
    "    \n",
    "    return vi_dict, pi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f72333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_discretize(the_map):\n",
    "    size = len(the_map)\n",
    "    dis_map = np.zeros((size,size))\n",
    "    for i, row in enumerate(the_map):\n",
    "        for j, loc in enumerate(row):\n",
    "            if loc == \"S\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"F\":\n",
    "                dis_map[i, j] = 0\n",
    "            elif loc == \"H\":\n",
    "                dis_map[i, j] = -1\n",
    "            elif loc == \"G\":\n",
    "                dis_map[i, j] = 1\n",
    "    return dis_map\n",
    "\n",
    "\n",
    "def policy_numpy(policy):\n",
    "    size = int(np.sqrt(len(policy)))\n",
    "    pol = np.asarray(policy)\n",
    "    pol = pol.reshape((size, size))\n",
    "    return pol\n",
    "\n",
    "\n",
    "def see_policy(map_size, policy):\n",
    "    map_name = str(map_size)+\"x\"+str(map_size)\n",
    "    data = map_discretize(MAPS[map_name])\n",
    "    np_pol = policy_numpy(policy)\n",
    "    plt.imshow(data, interpolation=\"nearest\")\n",
    "\n",
    "    for i in range(np_pol[0].size):\n",
    "        for j in range(np_pol[0].size):\n",
    "            arrow = '\\u2190'\n",
    "            if np_pol[i, j] == 1:\n",
    "                arrow = '\\u2193'\n",
    "            elif np_pol[i, j] == 2:\n",
    "                arrow = '\\u2192'\n",
    "            elif np_pol[i, j] == 3:\n",
    "                arrow = '\\u2191'\n",
    "            text = plt.text(j, i, arrow,\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c52f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_dict(dictionary, value=\"Score\", size=4, variable=\"Discount Rate\", log=False):\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    title = \"Average and Max {} on {}x{} Frozen Lake\".format(value, size, size)\n",
    "    the_val = value\n",
    "    value = \"Average {}\".format(the_val)\n",
    "    val_type = \"Type of {}\".format(the_val)\n",
    "    the_df = pd.DataFrame(columns=[variable, value, val_type])\n",
    "    for k, v in dictionary.items():\n",
    "        for val in v:\n",
    "            if not log:\n",
    "                dic = {variable: k, value: float(val), val_type: \"Average with std\"}\n",
    "            else:\n",
    "                dic = {variable: np.log10(k), value: float(val), val_type: \"Average with std\"}                \n",
    "            the_df = the_df.append(dic, ignore_index=True)\n",
    "        if not log:\n",
    "            dic = {variable: k, value: float(max(v)), val_type: \"Max\"}\n",
    "        else:\n",
    "            dic = {variable: np.log10(k), value: float(max(v)), val_type: \"Max\"}\n",
    "        the_df = the_df.append(dic, ignore_index=True)\n",
    "    sns.lineplot(x=variable, y=value, hue=val_type, style=val_type, markers=True, data=the_df).set(title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21d6bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_dict(the_dict):\n",
    "    \n",
    "    # return for discount\n",
    "    discount_rewards = {}\n",
    "    discount_iterations = {}\n",
    "    discount_times = {}\n",
    "\n",
    "\n",
    "    for disc in the_dict:\n",
    "        discount_rewards[disc] = []    \n",
    "        discount_iterations[disc] = []    \n",
    "        discount_times[disc] = []\n",
    "\n",
    "        for eps in the_dict[disc]:\n",
    "            discount_rewards[disc].append(the_dict[disc][eps]['mean_reward'])\n",
    "            discount_iterations[disc].append(the_dict[disc][eps]['iteration'])        \n",
    "            discount_times[disc].append(the_dict[disc][eps]['time_spent'].total_seconds())  \n",
    "\n",
    "            \n",
    "    epsilon_rewards = {}\n",
    "    epsilon_iterations = {}\n",
    "    epsilon_times = {}\n",
    "    for eps in the_dict[0.5]:\n",
    "        epsilon_rewards[eps] = []    \n",
    "        epsilon_iterations[eps] = []    \n",
    "        epsilon_times[eps] = []\n",
    "    \n",
    "        for disc in vi_dict:\n",
    "            epsilon_rewards[eps].append(the_dict[disc][eps]['mean_reward'])\n",
    "            epsilon_iterations[eps].append(the_dict[disc][eps]['iteration'])        \n",
    "            epsilon_times[eps].append(the_dict[disc][eps]['time_spent'].total_seconds()) \n",
    "            \n",
    "    return discount_rewards, discount_iterations, discount_times, epsilon_rewards, epsilon_iterations, epsilon_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0482552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in: 7 episodes and 0:00:00.001980 seconds\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m vi_dict, pi_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test_pi_vi\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9999\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m, in \u001b[0;36mtrain_and_test_pi_vi\u001b[0;34m(env, discount, epsilon, mute)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# run value iteration\u001b[39;00m\n\u001b[1;32m     11\u001b[0m vi_policy, vi_solve_iter, vi_solve_time \u001b[38;5;241m=\u001b[39m value_iteration(env, dis, eps)\n\u001b[0;32m---> 12\u001b[0m vi_mrews, vi_meps, _, __ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvi_policy\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     13\u001b[0m vi_dict[dis][eps][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m vi_mrews\n\u001b[1;32m     14\u001b[0m vi_dict[dis][eps][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_eps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m vi_meps\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mtest_policy\u001b[0;34m(env, policy, n_epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m ep \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m     11\u001b[0m     ep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(action)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Adjusting unpacking to match the output of env.step()\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "vi_dict, pi_dict = train_and_test_pi_vi(env, discount=[0.5, 0.75, 0.9, 0.95, 0.99, 0.9999], \n",
    "                                        epsilon=[1e-3, 1e-6, 1e-9, 1e-12, 1e-15], mute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efd9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1601ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (from gym) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yrizvi/miniconda3/envs/ML/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deedc61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gym[toy_text]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434db7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
